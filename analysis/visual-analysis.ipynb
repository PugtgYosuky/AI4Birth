{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual analysis of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import calibration\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 500\n",
    "# set xticks fontsize\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "# set yticks fontsize\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "# set labels fontsize\n",
    "plt.rcParams['axes.labelsize'] = 22\n",
    "# set legend fontsize\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "# set labels weight to bold\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "# set figure size\n",
    "plt.rcParams['figure.figsize'] = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = '../tests'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = experiment_dir.split('tests/')[1].replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'LogisticRegression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps, fps, tns, fns, roc_aucs = [], [], [], [], []\n",
    "for run in sorted(os.listdir(experiment_dir)):\n",
    "    run_dir = os.path.join(experiment_dir, run)\n",
    "    try:\n",
    "        with open(os.path.join(run_dir, 'results.json')) as f:\n",
    "            results = json.load(f)\n",
    "        roc_aucs += results[model]['roc_auc_score']\n",
    "        tps += results[model]['tp']\n",
    "        fps += results[model]['fp']\n",
    "        tns += results[model]['tn']\n",
    "        fns += results[model]['fn']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "cm = np.array([[np.mean(tns), np.mean(fps)], [np.mean(fns), np.mean(tps)]])\n",
    "std_cm = np.array([[np.std(tns), np.std(fps)], [np.std(fns), np.std(tps)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if std_cm[0][0] == 0:\n",
    "    group_counts = ['{0:0.2f}'.format(value) for value in cm.flatten()]\n",
    "    percentages_cm = (cm.T / cm.sum(axis=1)).T\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in percentages_cm.flatten()]\n",
    "else:\n",
    "    group_counts = ['{0:0.2f} ± {1:0.2f}'.format(value, std) for value, std in zip(cm.flatten(), std_cm.flatten())]\n",
    "    percentages_cm = (cm.T / cm.sum(axis=1)).T\n",
    "    # add percentages std\n",
    "    percentages_std = (std_cm.T / cm.sum(axis=1)).T\n",
    "    group_percentages = ['{0:.2%} ± {1:.2%}'.format(value, std) for value, std in zip(percentages_cm.flatten(), percentages_std.flatten())]\n",
    "#     group_percentages = ['{0:.2%}'.format(value) for value in percentages_cm.flatten()]\n",
    "\n",
    "labels = [f'{v1}\\n({v2})' for v1, v2 in zip(group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', annot_kws={\"size\": 22})\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.savefig(f'../images/{test_name}-{model}-CM.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-{model}-CM.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-{model}-CM.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUCROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['LogisticRegression', 'RandomForestClassifier', 'XGBClassifier', 'MLPClassifier', 'AdaBoostClassifier', 'SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_BINS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_values = {}\n",
    "for model in models:\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tpr_rates = []\n",
    "    roc_scores = []\n",
    "    prob_true_list = []\n",
    "    prob_pred_list = []\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    for exp in os.listdir(experiment_dir):\n",
    "        try:\n",
    "            preds_path = os.path.join(experiment_dir, exp, 'predictions')\n",
    "            models_preds = [f for f in os.listdir(preds_path) if model in f]\n",
    "            for preds_file in models_preds:\n",
    "                aux_path = os.path.join(preds_path, preds_file)\n",
    "                preds_df = pd.read_csv(aux_path)\n",
    "                # roc metric\n",
    "                roc_score = metrics.roc_auc_score(preds_df.y_true, preds_df.y_proba_1)\n",
    "                roc_scores.append(roc_score)\n",
    "                # roc curve\n",
    "                fpr_proba, tpr_proba, threshold_proba = metrics.roc_curve(preds_df.y_true, preds_df.y_proba_1)\n",
    "                interp_tpr = np.interp(mean_fpr, fpr_proba, tpr_proba)\n",
    "                interp_tpr[0] = 0.0\n",
    "                tpr_rates.append(interp_tpr)\n",
    "                # calibration tool\n",
    "                prob_true, prob_pred = calibration.calibration_curve(preds_df.y_true, preds_df.y_proba_1, n_bins=NUMBER_BINS)\n",
    "                # complete the list with nan if the length is less than NUMBER_BINS\n",
    "                if len(prob_true) < NUMBER_BINS:\n",
    "                    prob_true = np.concatenate((prob_true, np.full(NUMBER_BINS - len(prob_true), np.nan)))\n",
    "                    prob_pred = np.concatenate((prob_pred, np.full(NUMBER_BINS - len(prob_pred), np.nan)))\n",
    "                prob_true_list.append(prob_true)\n",
    "                prob_pred_list.append(prob_pred)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        mean_tpr = np.mean(tpr_rates, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "\n",
    "        mean_prob_true = np.nanmean(prob_true_list, axis=0)\n",
    "        mean_prob_pred = np.nanmean(prob_pred_list, axis=0)\n",
    "\n",
    "        preds_values[model] = {\n",
    "            'fpr' : mean_fpr,\n",
    "            'tpr' : mean_tpr,\n",
    "            'mean' : np.mean(roc_scores),\n",
    "            'std' : np.std(roc_scores),\n",
    "            'prob_true' : mean_prob_true,\n",
    "            'prob_pred' : mean_prob_pred\n",
    "        }\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort preds_values by mean\n",
    "preds_values = dict(sorted(preds_values.items(), key=lambda item: item[1]['mean'], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC CURVE ALL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = np.linspace(0, 1, 11)\n",
    "ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.set_style(\"whitegrid\")\n",
    "for model, result in preds_values.items():\n",
    "    lw = 3\n",
    "    plt.plot(\n",
    "        result['fpr'], \n",
    "        result['tpr'], \n",
    "        label=f'Mean ROC (AUC= {result[\"mean\"].round(3)} $\\pm$ {result[\"std\"].round(3)}) - {model}',\n",
    "        lw=lw)\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'g', label='Random Classifier')\n",
    "    \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "plt.grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.savefig(f'../images/{test_name}-ROC-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-ROC-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-ROC-models.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.set_style(\"whitegrid\")\n",
    "for model, result in preds_values.items():\n",
    "    lw = 3\n",
    "    if model == 'LogisticRegression':\n",
    "        plt.plot(\n",
    "            result['fpr'], \n",
    "            result['tpr'], \n",
    "            label=f'Mean ROC (AUC= {result[\"mean\"].round(3)} $\\pm$ {result[\"std\"].round(3)}) - {model}',\n",
    "            lw=lw)\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'g', label='Random Classifier')\n",
    "    \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "plt.grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.savefig(f'../images/{test_name}-ROC-lr.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-ROC-lr.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-ROC-lr.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color='black', label='Perfectly Calibrated')\n",
    "for model, result in preds_values.items():\n",
    "    lw = 3\n",
    "    plt.plot(\n",
    "        result['prob_pred'], \n",
    "        result['prob_true'], \n",
    "        label=f'{model}',\n",
    "        lw=lw, marker='o')\n",
    "    \n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('True Probability')\n",
    "# save legend outside the plot\n",
    "plt.legend()\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "plt.grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.savefig(f'../images/{test_name}-calibration-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-calibration-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-calibration-models.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color='black', label='Perfectly Calibrated')\n",
    "for model, result in preds_values.items():\n",
    "    lw = 3\n",
    "    if model == 'LogisticRegression':\n",
    "        plt.plot(\n",
    "            result['prob_pred'], \n",
    "            result['prob_true'], \n",
    "            label=f'{model}',\n",
    "            lw=lw, marker='o')\n",
    "    \n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('True Probability')\n",
    "# save legend outside the plot\n",
    "plt.legend()\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "plt.grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.savefig(f'../images/{test_name}-calibration-lr.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-calibration-lr.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/{test_name}-calibration-lr.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deliveries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
