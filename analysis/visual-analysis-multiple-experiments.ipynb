{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import calibration\n",
    "from dcurves import dca as decision_curve_analysis\n",
    "from utils import get_mean_results\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 500\n",
    "# set xticks fontsize\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "# set yticks fontsize\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "# set labels fontsize\n",
    "plt.rcParams['axes.labelsize'] = 22\n",
    "# set legend fontsize\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "# set labels weight to bold\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "# set rcparams for grid axs[1].grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.linestyle'] = '--'\n",
    "plt.rcParams['grid.alpha'] = 0.7\n",
    "plt.rcParams['grid.linewidth'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_BINS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'LogisticRegression'\n",
    "model_name = 'LR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    'cross-validation' : {\n",
    "        '../tests/test1/cv-PT' : 'LR - All Features',\n",
    "        '../tests/test1/cv-PT-reduced-13/' : 'LR - 13 Best Features',\n",
    "    },\n",
    "    'train-test' : {\n",
    "        '../tests/test1/train-PT-test-US' : 'LR - All Features',\n",
    "        '../tests/test1/train-PT-test-US-reduced-13' : 'LR - 13 Best Features',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_names = {\n",
    "    'cross-validation' : 'CV PT',\n",
    "    'train-test' : 'Train PT - Test CSL'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = np.linspace(0, 1, NUMBER_BINS + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get ROCs and Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for data_split, exps_paths in experiments.items():\n",
    "    preds_values = {}\n",
    "    for experiment_dir, experiment_name in exps_paths.items():\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        tpr_rates = []\n",
    "        roc_scores = []\n",
    "        prob_true_list = []\n",
    "        prob_pred_list = []\n",
    "        c_slopes = []\n",
    "        c_intercepts = []\n",
    "        # plt.figure(figsize=(10, 10))\n",
    "        for exp in os.listdir(experiment_dir):\n",
    "            try:\n",
    "                preds_path = os.path.join(experiment_dir, exp, 'predictions')\n",
    "                models_preds = [f for f in os.listdir(preds_path) if model in f]\n",
    "                for preds_file in models_preds:\n",
    "                    aux_path = os.path.join(preds_path, preds_file)\n",
    "                    preds_df = pd.read_csv(aux_path)\n",
    "                    # roc metric\n",
    "                    roc_score = metrics.roc_auc_score(preds_df.y_true, preds_df.y_proba_1)\n",
    "                    roc_scores.append(roc_score)\n",
    "                    # roc curve\n",
    "                    fpr_proba, tpr_proba, threshold_proba = metrics.roc_curve(preds_df.y_true, preds_df.y_proba_1)\n",
    "                    interp_tpr = np.interp(mean_fpr, fpr_proba, tpr_proba)\n",
    "                    interp_tpr[0] = 0.0\n",
    "                    tpr_rates.append(interp_tpr)\n",
    "\n",
    "                    # calibration curve\n",
    "                    prob_true, prob_pred = calibration.calibration_curve(preds_df.y_true, preds_df.y_proba_1, n_bins=NUMBER_BINS)\n",
    "                    slope, intercept = np.polyfit(prob_pred, prob_true, 1)\n",
    "                    c_slopes.append(slope)\n",
    "                    c_intercepts.append(intercept)\n",
    "                    # complete the list with nan if the length is less than NUMBER_BINS\n",
    "                    if len(prob_true) < NUMBER_BINS:\n",
    "                        prob_true = np.concatenate((prob_true, np.full(NUMBER_BINS - len(prob_true), np.nan)))\n",
    "                        prob_pred = np.concatenate((prob_pred, np.full(NUMBER_BINS - len(prob_pred), np.nan)))\n",
    "                    prob_true_list.append(prob_true)\n",
    "                    prob_pred_list.append(prob_pred)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            mean_tpr = np.mean(tpr_rates, axis=0)\n",
    "            mean_tpr[-1] = 1.0\n",
    "\n",
    "            mean_prob_true = np.nanmean(prob_true_list, axis=0)\n",
    "            mean_prob_pred = np.nanmean(prob_pred_list, axis=0)\n",
    "\n",
    "            preds_values[experiment_name] = {\n",
    "                'fpr' : mean_fpr,\n",
    "                'tpr' : mean_tpr,\n",
    "                'roc_auc_mean' : np.mean(roc_scores).round(3),\n",
    "                'roc_auc_std' : np.std(roc_scores).round(3),\n",
    "                'prob_true' : mean_prob_true,\n",
    "                'prob_pred' : mean_prob_pred,\n",
    "                'c_slopes_mean' : np.mean(c_slopes).round(3),\n",
    "                'c_slopes_std' : np.std(c_slopes).round(3),\n",
    "                'c_intercepts_mean' : np.mean(c_intercepts).round(3),\n",
    "                'c_intercepts_std' : np.std(c_intercepts).round(3),\n",
    "            }\n",
    "        except:\n",
    "            pass\n",
    "    data[data_split] = preds_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.set_style(\"whitegrid\")\n",
    "for model, result in data['cross-validation'].items():\n",
    "    lw = 3\n",
    "    plt.plot(\n",
    "        result['fpr'], \n",
    "        result['tpr'], \n",
    "        label=f'[ROC AUC= {result[\"roc_auc_mean\"]} $\\pm$ {result[\"roc_auc_std\"]}] {model}',\n",
    "        lw=lw)\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'grey', label='Random Classifier')\n",
    "    \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "plt.grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.savefig(f'../images/cross-validation-ROC-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/cross-validation-ROC-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/cross-validation-ROC-models.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.set_style(\"whitegrid\")\n",
    "for model, result in data['train-test'].items():\n",
    "    lw = 3\n",
    "    plt.plot(\n",
    "        result['fpr'], \n",
    "        result['tpr'], \n",
    "        label=f'(ROC AUC= {result[\"roc_auc_mean\"]} $\\pm$ {result[\"roc_auc_std\"]}) - {model}',\n",
    "        lw=lw)\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'grey', label='Random Classifier')\n",
    "    \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "plt.grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.savefig(f'../images/train-test-ROC-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/train-test-ROC-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/train-test-ROC-models.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 7))\n",
    "sns.set_style(\"whitegrid\")\n",
    "# cross validation\n",
    "for model, result in data['cross-validation'].items():\n",
    "    lw = 3\n",
    "    axs[0].plot(\n",
    "        result['fpr'], \n",
    "        result['tpr'], \n",
    "        label=f'[ROC AUC= {result[\"roc_auc_mean\"]} $\\pm$ {result[\"roc_auc_std\"]}] {model}',\n",
    "        lw=lw)\n",
    "axs[0].plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'grey', label='Random Classifier')\n",
    "axs[0].set_xlabel('False Positive Rate')\n",
    "axs[0].set_ylabel('True Positive Rate')\n",
    "axs[0].legend(loc='lower right')\n",
    "axs[0].set_title('Cross-Validation PT', fontsize=24, fontweight='bold')\n",
    "# train-test\n",
    "for model, result in data['train-test'].items():\n",
    "    lw = 3\n",
    "    axs[1].plot(\n",
    "        result['fpr'], \n",
    "        result['tpr'], \n",
    "        label=f'[ROC AUC= {result[\"roc_auc_mean\"]} $\\pm$ {result[\"roc_auc_std\"]}] {model}',\n",
    "        lw=lw)\n",
    "axs[1].plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'grey', label='Random Classifier')\n",
    "axs[1].set_xlabel('False Positive Rate')\n",
    "axs[1].set_ylabel('True Positive Rate')\n",
    "axs[1].legend(loc='lower right')\n",
    "axs[1].set_title(map_names['train-test'], fontsize=24, fontweight='bold')\n",
    "plt.savefig(f'../images/ROC-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/ROC-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/ROC-models.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calibration plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color='grey', label='Perfectly Calibrated')\n",
    "for model, result in data['cross-validation'].items():\n",
    "    lw = 3\n",
    "    plt.plot(\n",
    "        result['prob_pred'], \n",
    "        result['prob_true'], \n",
    "        label=f'[C-S={result[\"c_slopes_mean\"]}$\\pm${result[\"c_slopes_std\"]},C-I={result[\"c_intercepts_mean\"]}$\\pm${result[\"c_intercepts_std\"]}] {model}',\n",
    "        lw=lw, marker='o')\n",
    "    \n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('True Probability')\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "plt.grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(f'../images/cross-validation-calibration-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/cross-validation-calibration-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/cross-validation-calibration-models.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color='grey', label='Perfectly Calibrated')\n",
    "for model, result in data['train-test'].items():\n",
    "    lw = 3\n",
    "    plt.plot(\n",
    "        result['prob_pred'], \n",
    "        result['prob_true'], \n",
    "        label=f'[C-S={result[\"c_slopes_mean\"]},C-I={result[\"c_intercepts_mean\"]}] {model}',\n",
    "        lw=lw, marker='o')\n",
    "    \n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('True Probability')\n",
    "plt.xticks(ticks)\n",
    "plt.yticks(ticks)\n",
    "plt.grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.legend()\n",
    "plt.savefig(f'../images/train-test-calibration-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/train-test-calibration-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/train-test-calibration-models.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 7))\n",
    "sns.set_style(\"whitegrid\")\n",
    "# cross validation\n",
    "axs[0].plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'grey', label='Perfectly Calibrated')\n",
    "for model, result in data['cross-validation'].items():\n",
    "    lw = 3\n",
    "    axs[0].plot(\n",
    "        result['prob_pred'], \n",
    "        result['prob_true'], \n",
    "        label=f'[C-S={result[\"c_slopes_mean\"]},C-I={result[\"c_intercepts_mean\"]}] {model}',\n",
    "        lw=lw, marker='o')\n",
    "axs[0].set_xlabel('Predicted Probability')\n",
    "axs[0].set_ylabel('True Probability')\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set_title('Cross-Validation PT', fontsize=24, fontweight='bold')\n",
    "axs[0].grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "# train-test\n",
    "axs[1].plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'grey', label='Perfectly Calibrated')\n",
    "for model, result in data['train-test'].items():\n",
    "    lw = 3\n",
    "    lw = 3\n",
    "    axs[1].plot(\n",
    "        result['prob_pred'], \n",
    "        result['prob_true'], \n",
    "        label=f'[C-S={result[\"c_slopes_mean\"]},C-I={result[\"c_intercepts_mean\"]}] {model}',\n",
    "        lw=lw, marker='o')\n",
    "    \n",
    "axs[1].set_xlabel('Predicted Probability')\n",
    "axs[1].set_ylabel('True Probability')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set_title(map_names['train-test'], fontsize=24, fontweight='bold')\n",
    "axs[1].grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "plt.savefig(f'../images/calibration-curves-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/calibration-curves-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/calibration-curves-models.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm(experiment_dir, model_name):\n",
    "    tps, fps, tns, fns, roc_aucs = [], [], [], [], []\n",
    "    for run in sorted(os.listdir(experiment_dir)):\n",
    "        run_dir = os.path.join(experiment_dir, run)\n",
    "        try:\n",
    "            with open(os.path.join(run_dir, 'results.json')) as f:\n",
    "                results = json.load(f)\n",
    "            roc_aucs += results[model_name]['roc_auc_score']\n",
    "            tps += results[model_name]['tp']\n",
    "            fps += results[model_name]['fp']\n",
    "            tns += results[model_name]['tn']\n",
    "            fns += results[model_name]['fn']\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    cm = np.array([[np.mean(tns), np.mean(fps)], [np.mean(fns), np.mean(tps)]])\n",
    "    std_cm = np.array([[np.std(tns), np.std(fps)], [np.std(fns), np.std(tps)]])\n",
    "\n",
    "    if std_cm[0][0] == 0:\n",
    "        group_counts = ['{0:0.2f}'.format(value) for value in cm.flatten()]\n",
    "        percentages_cm = (cm.T / cm.sum(axis=1)).T\n",
    "        group_percentages = ['{0:.2%}'.format(value) for value in percentages_cm.flatten()]\n",
    "    else:\n",
    "        group_counts = ['{0:0.2f} ± {1:0.2f}'.format(value, std) for value, std in zip(cm.flatten(), std_cm.flatten())]\n",
    "        percentages_cm = (cm.T / cm.sum(axis=1)).T\n",
    "        # add percentages std\n",
    "        percentages_std = (std_cm.T / cm.sum(axis=1)).T\n",
    "        group_percentages = ['{0:.2%} ± {1:.2%}'.format(value, std) for value, std in zip(percentages_cm.flatten(), percentages_std.flatten())]\n",
    "    #     group_percentages = ['{0:.2%}'.format(value) for value in percentages_cm.flatten()]\n",
    "\n",
    "    labels = [f'{v1}\\n({v2})' for v1, v2 in zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    return cm, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_MODELS = 2\n",
    "NUMBER_TESTS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=NUMBER_TESTS, nrows=NUMBER_MODELS, figsize=(15, 10))\n",
    "for i, (name, exp_models) in enumerate(experiments.items()):\n",
    "    for j, (exp_dir, model_name) in enumerate(exp_models.items()):\n",
    "        cm, labels = get_cm(exp_dir, 'LogisticRegression')\n",
    "        sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', annot_kws={\"size\": 15, 'weight': 'bold'},  ax=axs[j, i])\n",
    "        axs[j, i].set_title(f'[{map_names[name]}] {model_name}'.replace('Combination', 'Comb'), fontsize=16, fontweight='bold')\n",
    "        axs[j, i].set_xlabel('Predicted Label', fontsize=14)\n",
    "        axs[j, i].set_ylabel('True Label', fontsize=14)\n",
    "        colorbar = axs[j, i].collections[0].colorbar\n",
    "        colorbar.ax.tick_params(labelsize=12)\n",
    "        # set ticks font size to 15\n",
    "        axs[j, i].tick_params(axis='both', which='major', labelsize=14)\n",
    "        axs[j, i].set_xticklabels(['VD', 'CS'])\n",
    "        axs[j, i].set_yticklabels(['VD', 'CS'])\n",
    "        # change vertical space between subplots\n",
    "        plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "plt.savefig(f'../images/CM-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/CM-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/CM-models.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Curve analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dca_values(path):\n",
    "    dca_values = {\n",
    "        'y_proba_1': [],\n",
    "        'all' : [],\n",
    "        'none' : [],\n",
    "        'thresholds' : []\n",
    "    }\n",
    "    for exp in os.listdir(path):\n",
    "        preds_path = os.path.join(path, exp, 'predictions')\n",
    "        for preds in [preds for preds in os.listdir(preds_path) if 'LogisticRegression' in preds]:\n",
    "            predictions = pd.read_csv(os.path.join(preds_path, preds))\n",
    "            dca_df = decision_curve_analysis(\n",
    "                data = predictions,\n",
    "                outcome = 'y_true',\n",
    "                modelnames = ['y_proba_1'],\n",
    "                thresholds = thresholds\n",
    "            )\n",
    "            # save values\n",
    "            for model in dca_df.model.unique():\n",
    "                dca_values[model].append(dca_df.loc[dca_df['model'] == model, 'net_benefit'].values)\n",
    "                dca_values['thresholds'].append(dca_df.loc[dca_df['model'] == model, 'threshold'].values)\n",
    "\n",
    "    for key, value in dca_values.items():\n",
    "        print(key)\n",
    "        dca_values[key] = np.mean(value, axis=0)\n",
    "    return dca_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {}\n",
    "for val_type, exps in experiments.items():\n",
    "    aux = {}\n",
    "    for path_dir, title in exps.items():\n",
    "        dca_values = get_dca_values(path_dir)\n",
    "        aux[title] = dca_values\n",
    "    values[val_type] = aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    'y_proba_1' : 'LR',\n",
    "    'all' : 'Treat as C-Section',\n",
    "    'none' : 'Treat as Vaginal Delivery'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dca_values = values['cross-validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dca_values['LR - All Features'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 7))\n",
    "for index, (val_type, value_info) in enumerate(values.items()):\n",
    "    dca_values = list(value_info.values())[0]\n",
    "    cmap = plt.get_cmap('Greys')\n",
    "    axs[index].plot(dca_values['thresholds'], dca_values['all'], label=f'{names[\"all\"]}', linewidth=2, color=cmap(0.8))\n",
    "    axs[index].plot(dca_values['thresholds'], dca_values['none'], label=f'{names[\"none\"]}', linewidth=2, color=cmap(0.6))\n",
    "    # setup colors\n",
    "    for title, dca_values in value_info.items():\n",
    "        axs[index].plot(dca_values['thresholds'], dca_values['y_proba_1'], label=f'{names[\"y_proba_1\"]} - {title}', linewidth=3)\n",
    "    axs[index].set_xlabel('Threshold')\n",
    "    axs[index].set_ylabel('Net Benefit')\n",
    "    axs[index].set_title(f'{map_names[val_type]}'.replace('CV', 'Cross-Validation'), fontsize=24, fontweight='bold')\n",
    "    axs[index].set_xticks(np.linspace(0, 1, 11))\n",
    "    axs[index].set_yticks(np.arange(-0.05, 0.35, 0.05))\n",
    "    axs[index].set_ylim([-0.05, 0.35])\n",
    "    axs[index].set_xlim([0, 1])\n",
    "    axs[index].legend(fontsize=13)\n",
    "    axs[index].grid(linestyle='--', alpha=0.7, linewidth=1)\n",
    "# plt.grid(linestyle='--', linewidth=1, alpha=0.7)\n",
    "plt.savefig(f'../images/DCA-models.png', bbox_inches='tight')\n",
    "plt.savefig(f'../images/DCA-models.svg', bbox_inches='tight')\n",
    "plt.savefig(f'../images/DCA-models.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deliveries",
   "language": "python",
   "name": "deliveries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
